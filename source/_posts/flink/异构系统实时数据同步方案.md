---
title: 异构系统实时数据同步方案
date: 2025-12-22 17:40:10
categories: flink
tag: 实时数据同步
---
这里为你编写一个基于 **Java Flink DataStream API** 的完整代码样例。

这种方式（Code 模式）比单纯的 SQL 模式更适合你提到的“数据对应关系不一样”和“一对多”的复杂业务逻辑，因为它允许你编写任意的 Java 代码来处理数据转换。

### 场景假设

假设我们有一个**源订单表**（一条记录包含多个商品ID），需要拆分同步到**目标详情表**（每个商品一条记录）。

* **源表 (MySQL - db_source.orders):**
* `id` (INT, PK)
* `customer_name` (VARCHAR)
* `product_ids` (VARCHAR) -> 例如: `"101,102,103"`


* **目标表 (MySQL - db_target.order_items):**
* `id` (INT, Auto Increment PK)
* `order_id` (INT)
* `customer_name` (VARCHAR)
* `single_product_id` (INT) -> 需要存 3 条记录: 101, 102, 103



---

### 1. Maven 依赖配置 (`pom.xml`)

你需要引入 Flink 核心库、Flink CDC 连接器和 JDBC 连接器。

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java</artifactId>
        <version>1.17.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients</artifactId>
        <version>1.17.1</version>
    </dependency>

    <dependency>
        <groupId>com.ververica</groupId>
        <artifactId>flink-connector-mysql-cdc</artifactId>
        <version>2.4.1</version>
    </dependency>

    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-jdbc</artifactId>
        <version>3.1.1-1.17</version>
    </dependency>
    
    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <version>8.0.28</version>
    </dependency>

    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.83</version>
    </dependency>
</dependencies>

```

---

### 2. 完整 Java 代码实现

这个程序由三个部分组成：**Source (CDC 读取)** -> **Transform (拆分逻辑)** -> **Sink (写入目标)**。

```java
import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONObject;
import com.ververica.cdc.connectors.mysql.source.MySqlSource;
import com.ververica.cdc.connectors.mysql.table.StartupOptions;
import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
import org.apache.flink.connector.jdbc.JdbcSink;
import org.apache.flink.connector.jdbc.JdbcStatementBuilder;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

import java.sql.PreparedStatement;
import java.sql.SQLException;

// 1. 定义目标数据的实体类 (POJO)
class TargetOrderItem {
    public int orderId;
    public String customerName;
    public int productId;

    public TargetOrderItem(int orderId, String customerName, int productId) {
        this.orderId = orderId;
        this.customerName = customerName;
        this.productId = productId;
    }
}

public class MySqlSyncJob {
    public static void main(String[] args) throws Exception {
        
        // 1. 获取执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1); // 生产环境根据数据量调整，测试用 1

        // =================================================================
        // STEP 1: 定义 Source (Flink CDC)
        // =================================================================
        MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
                .hostname("localhost")
                .port(3306)
                .databaseList("db_source") // 监控的数据库
                .tableList("db_source.orders") // 监控的表
                .username("root")
                .password("123456")
                // initial() 会先读取全量历史数据，然后切换到 Binlog 读取增量
                .startupOptions(StartupOptions.initial()) 
                .deserializer(new JsonDebeziumDeserializationSchema()) // 转成 JSON 字符串处理
                .build();

        DataStreamSource<String> rawStream = env.fromSource(
                mySqlSource, 
                WatermarkStrategy.noWatermarks(), 
                "MySQL CDC Source"
        );

        // =================================================================
        // STEP 2: Transform (核心业务逻辑：1转N)
        // =================================================================
        SingleOutputStreamOperator<TargetOrderItem> processedStream = rawStream.flatMap(new FlatMapFunction<String, TargetOrderItem>() {
            @Override
            public void flatMap(String value, Collector<TargetOrderItem> out) throws Exception {
                try {
                    // 1. 解析 CDC 的 JSON 数据
                    JSONObject json = JSON.parseObject(value);
                    
                    // CDC JSON 结构通常包含 "before", "after", "op" (操作类型)
                    // 我们主要关注 "after" (变更后的数据) 和 "op"
                    // op: c (create), u (update), d (delete), r (read/snapshot)
                    
                    JSONObject after = json.getJSONObject("after");
                    String op = json.getString("op");

                    // 简单起见，这里只处理新增(c, r)和更新(u)，忽略删除(d)
                    // 如果 after 为 null (例如删除操作)，则跳过
                    if (after == null) {
                        return;
                    }

                    // 2. 提取源字段
                    Integer orderId = after.getInteger("id");
                    String customerName = after.getString("customer_name");
                    String productIdsStr = after.getString("product_ids"); // "101,102,103"

                    // 3. 执行 1转N 的业务逻辑
                    if (productIdsStr != null && !productIdsStr.isEmpty()) {
                        String[] ids = productIdsStr.split(",");
                        for (String id : ids) {
                            // 构造目标对象
                            TargetOrderItem item = new TargetOrderItem(
                                    orderId,
                                    customerName,
                                    Integer.parseInt(id.trim())
                            );
                            // 发送往下游
                            out.collect(item);
                        }
                    }

                } catch (Exception e) {
                    // 生产环境建议记录错误日志到侧输出流(SideOutput)，不要直接抛出导致任务挂掉
                    System.err.println("解析错误: " + value);
                    e.printStackTrace();
                }
            }
        });

        // =================================================================
        // STEP 3: Sink (写入目标数据库)
        // =================================================================
        String insertSql = "INSERT INTO order_items (order_id, customer_name, single_product_id) VALUES (?, ?, ?)";
        
        // 如果需要幂等性（防止重复写入），建议使用 "INSERT ... ON DUPLICATE KEY UPDATE"
        // String upsertSql = "INSERT INTO order_items ... VALUES ... ON DUPLICATE KEY UPDATE ...";

        processedStream.addSink(JdbcSink.sink(
                insertSql,
                new JdbcStatementBuilder<TargetOrderItem>() {
                    @Override
                    public void accept(PreparedStatement ps, TargetOrderItem item) throws SQLException {
                        ps.setInt(1, item.orderId);
                        ps.setString(2, item.customerName);
                        ps.setInt(3, item.productId);
                    }
                },
                JdbcExecutionOptions.builder()
                        .withBatchSize(100)          // 批处理大小，提高性能
                        .withBatchIntervalMs(200)    // 批处理间隔
                        .withMaxRetries(3)           // 重试次数
                        .build(),
                new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                        .withUrl("jdbc:mysql://localhost:3306/db_target")
                        .withDriverName("com.mysql.cj.jdbc.Driver")
                        .withUsername("root")
                        .withPassword("123456")
                        .build()
        ));

        // 4. 启动任务
        env.execute("MySQL Sync Job 1-to-N");
    }
}

```

### 关键点解析

1. **JsonDebeziumDeserializationSchema**:
* CDC 捕获的数据是复杂的 Debezium 格式。使用这个反序列化器，你会得到类似这样的 JSON 字符串：
```json
{
  "before": null,
  "after": { "id": 1, "customer_name": "Alice", "product_ids": "101,102" },
  "source": { ... },
  "op": "c",
  "ts_ms": 16788888888
}

```


* 在 `FlatMap` 中，我们解析 `after` 字段拿到最新的数据。


2. **FlatMap 算子 (核心)**:
* 这就是你实现“任意业务逻辑”的地方。
* 你可以写 `if-else`，可以查缓存，可以做字符串 `split`，完全自由。
* `out.collect(item)` 可以被调用多次，从而实现 **1 条源数据 -> N 条目标数据**。


3. **Sink 性能优化**:
* `JdbcSink` 支持 `withBatchSize`。这非常重要，它不会每来一条数据就插库一次，而是攒够比如 100 条或者过了 200ms 再批量插入，能极大提升吞吐量。



### 如何运行

1. **准备数据库**: 在本地 MySQL 创建好 `db_source.orders` 和 `db_target.order_items`。
2. **开启 Binlog**: 确保源 MySQL 开启了 Binlog (`log_bin=mysql-bin`, `binlog_format=ROW`)。
3. **运行代码**: 在 IDE 中直接运行 `main` 方法。
4. **测试**:
* 向 `db_source.orders` 插入一条: `INSERT INTO orders VALUES (1, 'Bob', 'A,B,C');`
* 查看控制台无报错。
* 查看 `db_target.order_items`，应该瞬间出现 3 条记录 (Bob-A, Bob-B, Bob-C)。
